{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_comment_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPR6gZ2hVZqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "389c4371-51a9-4e25-99ca-a85af7b2ceed"
      },
      "source": [
        "#Mounting google drive to access train,test data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fNTXrtxfzvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Relevant Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRNuDe9sgYxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upload train,test,sample submission dataset files\n",
        "train_path = \"/content/drive/My Drive/data/train.csv\"\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "test_path = \"/content/drive/My Drive/data/test.csv\"\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "sample_submission_path = \"/content/drive/My Drive/data/sample_submission.csv\"\n",
        "df_sample_submission = pd.read_csv(sample_submission_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UwJxBFWml2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "b9658ec5-65eb-487b-f01d-9e2339ba461d"
      },
      "source": [
        "#EDA\n",
        "#Data shape\n",
        "print(\"Train data has\",df_train.shape[0],\"rows and\",df_train.shape[1],\"columns.\")\n",
        "print(\"Test data has\",df_test.shape[0],\"rows and\",df_test.shape[1],\"columns.\")\n",
        "print(\"Sample submission data has\",df_sample_submission.shape[0],\"rows and\",df_sample_submission.shape[1],\"columns.\")\n",
        "\n",
        "df_train.describe()\n",
        "\n",
        "#Eye roll the columns\n",
        "df_train.head(5)\n",
        "df_test.head(5)\n",
        "df_sample_submission.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP8EdIKTAdcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking column info\n",
        "df_train.info()\n",
        "df_test.info()\n",
        "\n",
        "#check for missing value in Train dataset\n",
        "train_null_check = df_train['comment_text'].isnull().sum()\n",
        "print(\"train set missing values\")\n",
        "print(train_null_check)\n",
        "#check for missing value in Test dataset\n",
        "test_null_check = df_test['comment_text'].isnull().sum()\n",
        "print(\"test set missing values\")\n",
        "print(test_null_check)\n",
        "\n",
        "#drop observations if missing value exists\n",
        "if train_null_check > 0:\n",
        "  df_train.dropna(inplace=True)\n",
        "\n",
        "if test_null_check > 0:\n",
        "  df_test.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDQhMvO00Jxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7aa2f98b-8fca-46e6-a38a-56cfca893fcf"
      },
      "source": [
        "#check & drop for empty/space strings in 'comment_text' column\n",
        "def empty_text_check(df):\n",
        "  for index,id,comment in df.itertuples():  # iterate over the DataFrame\n",
        "    if type(comment)==str:            # avoid NaN values\n",
        "      if comment.isspace():         # test 'comment_text' for whitespace\n",
        "        blanks.append(index)     # add matching index numbers to the list\n",
        "\n",
        "#train\n",
        "blanks = []\n",
        "empty_text_check(df_train[['id','comment_text']])\n",
        "print(len(blanks), 'train blanks: ', blanks)\n",
        "if len(blanks) > 0:\n",
        "  df_train.drop(blanks, inplace=True)\n",
        "\n",
        "#test\n",
        "blanks = []\n",
        "empty_text_check(df_test[['id','comment_text']])\n",
        "print(len(blanks), 'test blanks: ', blanks)\n",
        "if len(blanks) > 0:\n",
        "  df_test.drop(blanks, inplace=True)\n",
        "\n",
        "print(df_test.shape)\n",
        "print(df_train.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 test blanks:  [55142]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxtqfeoHipX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking imbalance (clean vs non-clean)\n",
        "#checking the number of observations without any tagging for toxicity (clean) \n",
        "rowise_sum = df_train.iloc[:,2:].sum(axis=1)\n",
        "df_train['clean'] = (rowise_sum==0)\n",
        "print(\"Total comments in training set:\",len(df_train))\n",
        "print(\"Clean comments in training set:\",df_train['clean'].sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkJEgCkWLFl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train = df_train.drop('clean',axis=1)\n",
        "#columnwise label's count analysis\n",
        "df_train = df_train.drop('clean',axis = 1)\n",
        "train_label_count = df_train.iloc[:,2:].sum()\n",
        "train_label_count\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "train_count_plot = sns.barplot(train_label_count.index, train_label_count.values,palette=\"deep\")\n",
        "plt.title(\"Label Counts\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN4_mSldPorr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initial observation of dataframe showed that a single observation can have multiple labels also.\n",
        "#i.e we have multi-label classification, hence check correlation\n",
        "corr = df_train.corr()\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.heatmap(corr,cmap=\"YlGnBu\", xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True)\n",
        "\n",
        "#To do: instead of using the 'corr', we can try crosstab, confusion matrix."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_vmbwPqcKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Engineering\n",
        "#In multi-label classification we can use Problem transformation technique\n",
        "#(1) Binary Relevance (2)Classifier Chains (3)Label Powerset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnRwOii3rQax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Note: For reference we pull out accuracy of each label individually\n",
        "#and use it as base accuracy (i.e if our model predicted correctly all the tagged labels and failed to predict untagged ones)\n",
        "target_labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "train_base_acc_score = {}\n",
        "print(\"Target Labelwise Train Base Accuracy : \")\n",
        "for i in target_labels:\n",
        "    acc = ((df_train[i] == 0).sum()/len(df_train[i]))*100\n",
        "    print(i+':' ,format(acc))\n",
        "    train_base_acc_score[i] = acc\n",
        "\n",
        "train_base_acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEQTMFuJsFtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining a function for cleaning of text\n",
        "def clean_text(original_text):\n",
        "    cleaned_text = original_text.lower()\n",
        "    cleaned_text = re.sub(r\"\\'scuse\", \" excuse \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"i'm\", \"i am \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ll\", \" will \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"what's\", \"what is \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'s\", \" \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ve\", \" have \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"can't\", \"cannot \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"n't\", \" not \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'re\", \" are \", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'d\", \" would \", cleaned_text)\n",
        "    cleaned_text = re.sub('\\W', ' ', cleaned_text)\n",
        "    cleaned_text = re.sub('\\s+', ' ', cleaned_text)\n",
        "    cleaned_text = cleaned_text.strip(' ')\n",
        "    return cleaned_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qmb5F6ntbiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# cleaning the column comment_text in df_train\n",
        "cleaned_comment_train = []\n",
        "for i in range(0,len(df_train)):\n",
        "    cleaned_comment = clean_text(df_train['comment_text'][i])\n",
        "    cleaned_comment_train.append(cleaned_comment)\n",
        "df_train['comment_text'] = pd.Series(cleaned_comment_train).astype(str)\n",
        "\n",
        "# cleaning the column comment_text in df_test\n",
        "cleaned_comment_test = []\n",
        "for i in range(0,len(df_test)):\n",
        "    cleaned_comment = clean_text(df_test['comment_text'][i])\n",
        "    cleaned_comment_test.append(cleaned_comment)\n",
        "df_test['comment_text'] = pd.Series(cleaned_comment_test).astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPEMzO0U0QXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transforming our input data into a TF-IDF matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer(max_features=20000,stop_words='english')\n",
        "vect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyxbazYA08uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Assigning 'x' for train and test to be used for document term matrix(dtm)\n",
        "x_train = df_train.comment_text\n",
        "x_test = df_test.comment_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCxyLdckJio4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dtm for train\n",
        "x_train_dtm = vect.fit_transform(x_train)\n",
        "x_train_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJoWOnJpKKXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dtm for test\n",
        "x_test_dtm = vect.transform(x_test)\n",
        "x_test_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JgeVxxXKdx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upto here we have done the required data cleaning and transformation\n",
        "#now we start with training the model using sklearn library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lma3MU8tK4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#(1) Binary Relevance using Logistic Regression\n",
        "#using sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr = LogisticRegression(C=12.0)\n",
        "lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yczkDDk8-MQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using For Loop for binary relevance technique\n",
        "train_pred_acc_score = {}\n",
        "for label in target_labels:\n",
        "  y_train = df_train[label]\n",
        "  #fitting the model\n",
        "  lr.fit(x_train_dtm, y_train)\n",
        "  #predicting on train\n",
        "  y_pred = lr.predict(x_train_dtm)\n",
        "  #storing accuracy of training\n",
        "  train_pred_acc_score[label] = accuracy_score(y_train, y_pred)*100\n",
        "\n",
        "  #predicting on test\n",
        "  y_test_pred = lr.predict(x_test_dtm)\n",
        "  #getting probability\n",
        "  y_test_probability = lr.predict_proba(x_test_dtm)[:,1]\n",
        "  #storing in submission dataframe\n",
        "  df_sample_submission[label] = y_test_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REcO5aYGBVii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we can compare the train accuracy with reference base accuracy\n",
        "print(\"Train prediction accuracy:\")\n",
        "train_pred_acc_score\n",
        "print(\"Train base accuracy:\")\n",
        "train_base_acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7vf4TjTUJU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample_submission.head(2)\n",
        "df_sample_submission.to_csv('/content/drive/My Drive/data/final_submission_binary_relevance.csv',\n",
        "                            index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpayFjrGVddi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##(2) Classifier Chains using Logistic Regression\n",
        "# creating a function to add labels/features iterationwise\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "def add_label(x, label):\n",
        "    \n",
        "    return hstack([x, csr_matrix(label).T], 'csr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrGg0JsXybDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for label in target_labels:\n",
        "\n",
        "  y_train = df_train[label]\n",
        "  #fitting the model\n",
        "  lr.fit(x_train_dtm, y_train)\n",
        "  #predicting on train\n",
        "  y_pred = lr.predict(x_train_dtm)\n",
        "  #storing accuracy of training\n",
        "  train_pred_acc_score[label] = accuracy_score(y_train, y_pred)*100\n",
        "  #predicting on test\n",
        "  y_test_pred = lr.predict(x_test_dtm)\n",
        "  #getting probability\n",
        "  y_test_probability = lr.predict_proba(x_test_dtm)[:,1]\n",
        "  #storing in submission dataframe\n",
        "  df_sample_submission[label] = y_test_probability\n",
        "\n",
        "  #chain current label to X_dtm\n",
        "  x_train_dtm = add_label(x_train_dtm, y_train)\n",
        "    \n",
        "  # chain current label predictions to test_X_dtm\n",
        "  x_test_dtm = add_label(x_test_dtm, y_test_pred)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UCM6mvq1ZTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check if train accuracy has increased as compared to binary relevance\n",
        "print(\"Train prediction accuracy:\")\n",
        "train_pred_acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM8-k8ds1UUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample_submission.head(2)\n",
        "df_sample_submission.to_csv('/content/drive/My Drive/data/final_submission_classifier_chain.csv',\n",
        "                            index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzG7MYhj6CP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#confusion matrix\n",
        "print(\"Confusion Matrix : \")\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "print(\"Classification Report : \")\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "#AUC score\n",
        "modelauc_score = roc_auc_score(y_train, y_pred)\n",
        "print(\"AUC :\",modelauc_score)\n",
        "# calculate roc curves\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_train, y_pred)\n",
        "# plot the roc curve for the model\n",
        "plt.plot(lr_fpr, lr_tpr,marker='.',label='Classifier Chain Logistic Regression')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "plt.legend()\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}